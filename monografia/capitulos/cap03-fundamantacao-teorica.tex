\chapter{FUNDAMENTAÇÃO TEÓRICA}
\label{cap:fundamentacao}

O presente capítulo tem como objetivo apresentar a fundamentação teórica que embasa este trabalho, por meio da revisão de conceitos, metodologias e tecnologias relacionadas ao tema em estudo. Serão abordados os principais referenciais da literatura científica, contemplando definições, princípios e estudos já consolidados, a fim de oferecer suporte teórico para a compreensão do problema e a construção da solução proposta.

\section{Engenharia de Software}
\label{sec:eng-software}

A Engenharia de Software emerge como disciplina estruturada em resposta à chamada "Crise do Software" da década de 1960, quando o aumento da complexidade dos sistemas evidenciou a necessidade de métodos sistemáticos para o desenvolvimento, operação e manutenção de software (PRESSMAN; MAXIM, 2021). Diferentemente da programação isolada, a engenharia de software estabelece processos, técnicas e ferramentas que garantem a construção de sistemas confiáveis, eficientes e que atendam às necessidades dos usuários. Seu objetivo central é produzir software de qualidade dentro de prazos e custos estabelecidos, considerando não apenas a codificação, mas todo o ciclo de vida do produto, desde a concepção até a manutenção (SOMMERVILLE, 2019). Historicamente, o campo evoluiu de modelos prescritivos e sequenciais, como o modelo Cascata, para abordagens iterativas e incrementais, culminando nas metodologias ágeis contemporâneas, que valorizam a adaptação rápida a mudanças e a colaboração contínua com stakeholders. Essa evolução reflete a necessidade de maior flexibilidade diante da dinâmica acelerada do mercado tecnológico e das expectativas dos usuários por entregas frequentes e incrementais de valor (BECK et al., 2001, tradução nossa). A engenharia de software, portanto, não se limita a aspectos técnicos, mas engloba gestão de pessoas, processos e tecnologias, sendo fundamental para o sucesso de projetos de qualquer escala ou complexidade.

\subsection{Levantamento de Requisitos}
\label{subsec:levantamento-requisitos}

O levantamento de requisitos constitui a fase inicial e crítica do desenvolvimento de software, na qual são identificadas e documentadas as necessidades e expectativas dos stakeholders em relação ao sistema a ser construído (SOMMERVILLE, 2019). Requisitos são declarações que descrevem o que o sistema deve realizar e sob quais condições deve operar, sendo tradicionalmente categorizados em requisitos funcionais e não funcionais. Os requisitos funcionais especificam as funcionalidades que o sistema deve oferecer, tais como operações de cadastro, consulta, processamento de dados e geração de relatórios, representando as ações observáveis que o sistema executa em resposta a entradas ou eventos (PRESSMAN; MAXIM, 2021). Já os requisitos não funcionais estabelecem critérios de qualidade que o sistema deve satisfazer, abrangendo aspectos como desempenho, segurança, usabilidade, escalabilidade, confiabilidade e manutenibilidade. Esses requisitos definem restrições sobre como o sistema implementa suas funcionalidades, sendo muitas vezes determinantes para a satisfação dos usuários e o sucesso da aplicação em ambientes reais. A elicitação de requisitos demanda o emprego de técnicas estruturadas que facilitem a comunicação entre desenvolvedores e stakeholders. Entre as técnicas amplamente utilizadas destacam-se entrevistas, que permitem obter informações detalhadas diretamente de usuários e especialistas do domínio; questionários, adequados para coletar dados de um número maior de participantes de forma padronizada; workshops colaborativos, nos quais diferentes partes interessadas discutem e negociam requisitos em conjunto; prototipação, que viabiliza a validação precoce de conceitos por meio de versões simplificadas do sistema; e análise de documentação existente, útil quando há sistemas legados ou processos já estabelecidos (SOMMERVILLE, 2019). A escolha das técnicas varia conforme o contexto do projeto, a natureza do domínio e a disponibilidade dos stakeholders. A documentação clara e precisa dos requisitos, frequentemente formalizada em especificações de requisitos de software, é essencial para evitar retrabalho e garantir que o produto final atenda às expectativas, constituindo a base para todas as etapas subsequentes do desenvolvimento (PRESSMAN; MAXIM, 2021).

\subsection{Modelagem do Sistema}
\label{subsec:modelagem-sistema}

A modelagem de sistemas de software consiste na criação de representações abstratas que facilitam a compreensão, a comunicação e a análise da estrutura e do comportamento do sistema antes de sua implementação. A \emph{Unified Modeling Language} (UML), consolidada como padrão internacional pela \emph{Object Management Group} em meados dos anos 1990, oferece um conjunto rico de diagramas para modelagem de sistemas orientados a objetos (BOOCH; RUMBAUGH; JACOBSON, 2005, tradução nossa). A UML organiza-se em diagramas estruturais, que representam aspectos estáticos do sistema, e diagramas comportamentais, que capturam sua dinâmica e interações. Entre os diagramas mais relevantes no contexto de engenharia de software, destacam-se o Diagrama de Casos de Uso, o Diagrama de Classes, o Diagrama de Sequência e o Diagrama de Atividades, cada um atendendo a propósitos específicos no processo de análise e projeto. O Diagrama de Casos de Uso documenta as funcionalidades do sistema sob a perspectiva dos usuários finais, identificando atores (agentes externos que interagem com o sistema) e casos de uso (funcionalidades específicas fornecidas pelo sistema). Esse diagrama é particularmente útil na fase de levantamento de requisitos, pois fornece uma visão de alto nível do escopo funcional e facilita a comunicação com stakeholders não técnicos (SOMMERVILLE, 2019). O Diagrama de Classes, por sua vez, representa a estrutura estática do sistema, especificando as classes que compõem a solução, seus atributos, métodos e os relacionamentos entre elas, como associações, agregações, composições e heranças. Este diagrama é fundamental para o projeto orientado a objetos, pois define a arquitetura lógica do sistema e orienta a implementação (FOWLER, 2003, tradução nossa). O Diagrama de Sequência ilustra a interação temporal entre objetos ao longo da execução de um caso de uso ou cenário específico, evidenciando a ordem em que mensagens são trocadas e os métodos invocados. Esse tipo de modelagem é valioso para compreender fluxos de controle complexos e para identificar dependências entre componentes (BOOCH; RUMBAUGH; JACOBSON, 2005, tradução nossa). Finalmente, o Diagrama de Atividades modela o fluxo de trabalho ou processos dentro do sistema, representando atividades, decisões, bifurcações e sincronizações. Esse diagrama é útil tanto para documentar processos de negócio quanto para descrever a lógica interna de operações complexas, apoiando a compreensão de fluxos não lineares e condições de execução (FOWLER, 2003, tradução nossa). Em conjunto, esses diagramas UML constituem uma base sólida para a documentação arquitetural e facilitam a transição da análise de requisitos para o projeto detalhado e a implementação.

\subsection{Arquitetura e Tecnologias}
\label{subsec:arquitetura-tecnologias}

A arquitetura de software refere-se à estrutura fundamental de um sistema, definindo seus componentes principais, as propriedades externamente visíveis desses componentes e os relacionamentos entre eles (BASS; CLEMENTS; KAZMAN, 2021, tradução nossa). A escolha arquitetural tem impacto direto sobre atributos de qualidade como desempenho, escalabilidade, manutenibilidade e segurança, sendo uma das decisões mais críticas no ciclo de desenvolvimento. Diferentes estilos arquiteturais oferecem trade-offs específicos e são adequados a contextos distintos. A arquitetura cliente-servidor, por exemplo, separa responsabilidades entre clientes que solicitam serviços e servidores que os proveem, promovendo centralização de dados e lógica de negócio. Já a arquitetura em camadas organiza o sistema em níveis hierárquicos, onde cada camada oferece serviços à camada superior e consome serviços da camada inferior, facilitando a separação de responsabilidades e a manutenibilidade (PRESSMAN; MAXIM, 2021). A arquitetura de microsserviços, popularizada nas últimas duas décadas, propõe a decomposição do sistema em serviços pequenos, independentes e fracamente acoplados, cada um responsável por uma funcionalidade específica. Essa abordagem favorece escalabilidade horizontal, resiliência e facilitação de deploys frequentes, alinhando-se a práticas DevOps e ao desenvolvimento ágil (NEWMAN, 2021, tradução nossa). Além da estrutura arquitetural, a seleção de tecnologias é igualmente determinante para o sucesso do projeto. A escolha de linguagens de programação, frameworks, sistemas gerenciadores de banco de dados e ferramentas de desenvolvimento deve considerar fatores como maturidade da tecnologia, disponibilidade de bibliotecas e comunidade ativa, compatibilidade com requisitos não funcionais e expertise da equipe de desenvolvimento (BASS; CLEMENTS; KAZMAN, 2021, tradução nossa). Decisões arquiteturais mal fundamentadas podem resultar em sistemas difíceis de manter, com baixo desempenho ou incapazes de escalar adequadamente, evidenciando a importância de uma análise criteriosa e documentada desde as fases iniciais do projeto.

\subsection{Metodologias e Processos de Desenvolvimento}
\label{subsec:metodologias-e-processos-de-desenvolvimento}

As metodologias de desenvolvimento de software definem processos estruturados que orientam as atividades de concepção, construção, teste e entrega de sistemas. Historicamente, predominaram abordagens prescritivas e sequenciais, como o modelo Cascata, no qual o desenvolvimento ocorre em fases bem delimitadas (levantamento de requisitos, projeto, implementação, teste e manutenção), com entregas únicas ao final do ciclo (PRESSMAN; MAXIM, 2021). Embora proporcione previsibilidade e documentação extensa, esse modelo revela-se pouco adaptável a mudanças, sendo mais adequado a projetos com requisitos estáveis e bem compreendidos desde o início. O modelo Espiral, proposto por Boehm, introduz iterações e análise de riscos, mas ainda mantém características de planejamento extenso e documentação detalhada (SOMMERVILLE, 2019). A partir do final dos anos 1990, metodologias ágeis emergiram como resposta às limitações dos modelos tradicionais, priorizando indivíduos e interações sobre processos e ferramentas, software funcionando sobre documentação abrangente, colaboração com o cliente sobre negociação contratual e resposta a mudanças sobre seguir um plano (BECK et al., 2001, tradução nossa). Entre as metodologias ágeis mais difundidas, destaca-se o Scrum, um framework iterativo e incremental que organiza o trabalho em sprints de duração fixa, promovendo entregas frequentes, feedback contínuo e adaptação rápida a mudanças de requisitos. O Extreme Programming (XP) enfatiza práticas técnicas como programação em pares, desenvolvimento orientado a testes e integração contínua, visando elevar a qualidade do código e a satisfação do cliente. Já o Kanban foca na visualização do fluxo de trabalho e na limitação do trabalho em progresso, otimizando a eficiência e reduzindo gargalos (BECK; ANDRES, 2004, tradução nossa). Complementarmente, o Design Thinking pode ser integrado ao processo de desenvolvimento como abordagem centrada no ser humano para identificação de problemas e geração de soluções inovadoras, promovendo empatia com usuários e prototipação rápida (BROWN, 2008, tradução nossa). A escolha da metodologia mais adequada depende do contexto do projeto, da natureza dos requisitos, da maturidade da equipe e das expectativas dos stakeholders, sendo comum a adoção de práticas híbridas que combinam elementos de diferentes abordagens.

\subsection{Validação e Testes}
\label{subsec:validauxe7uxe3o-e-testes}

A validação e verificação de software são atividades essenciais para assegurar que o sistema construído atende aos requisitos especificados e está livre de defeitos que comprometam seu funcionamento. A verificação responde à questão "estamos construindo o produto corretamente?", enquanto a validação responde "estamos construindo o produto correto?" (SOMMERVILLE, 2019). Testes de software constituem o principal mecanismo de validação e verificação, sendo classificados em diferentes níveis conforme o escopo de aplicação. Os testes unitários focam na validação de componentes individuais, como funções ou classes, isoladamente, verificando se cada unidade produz os resultados esperados para entradas específicas. Testes de integração avaliam a interação entre módulos ou componentes, identificando falhas na comunicação entre partes do sistema. Testes de sistema validam o comportamento do sistema completo em relação aos requisitos funcionais e não funcionais, enquanto testes de aceitação verificam se o software atende às expectativas dos usuários finais e stakeholders, sendo frequentemente conduzidos em ambientes reais de operação (PRESSMAN; MAXIM, 2021). Além da classificação por nível, os testes podem ser categorizados quanto à técnica empregada. Testes de caixa-branca, também conhecidos como testes estruturais, examinam a lógica interna do código, buscando exercitar diferentes caminhos de execução, condições e laços. Testes de caixa-preta, ou testes funcionais, concentram-se nas entradas e saídas do sistema, sem considerar sua estrutura interna, sendo baseados exclusivamente nas especificações de requisitos (SOMMERVILLE, 2019). A prototipação, embora não seja um teste formal, desempenha papel relevante na validação precoce de conceitos e na obtenção de feedback dos usuários antes da implementação completa, reduzindo riscos de retrabalho (PRESSMAN; MAXIM, 2021). A execução eficaz de testes é suportada por ferramentas automatizadas que aumentam a produtividade e a confiabilidade do processo. Frameworks como JUnit para Java, Jest para JavaScript e Pytest para Python facilitam a criação e execução de testes unitários. Para testes de interface de usuário, ferramentas como Selenium e Cypress permitem automatizar interações com elementos visuais. Testes de APIs são comumente realizados com ferramentas como Postman e Insomnia, que validam requisições HTTP e respostas de serviços web. Métricas de qualidade, como cobertura de testes (percentual de código exercitado pelos testes) e densidade de defeitos (número de defeitos por mil linhas de código), auxiliam na avaliação objetiva da qualidade do software e na tomada de decisões sobre sua liberação para produção (SOMMERVILLE, 2019). Em suma, um processo rigoroso de validação e testes é indispensável para a entrega de software confiável, seguro e alinhado às expectativas dos usuários.

\section{Linguagem Java}
\label{sec:linguagem-java}

A linguagem de programação Java, lançada pela Sun Microsystems em 1995, consolidou-se como uma das tecnologias mais influentes no desenvolvimento de software contemporâneo, sendo amplamente adotada em aplicações empresariais, sistemas embarcados e dispositivos móveis (DEITEL; DEITEL, 2017). Concebida com o lema "Write Once, Run Anywhere" (Escreva uma vez, execute em qualquer lugar), Java introduziu o conceito de portabilidade por meio da Java Virtual Machine (JVM), que permite que programas compilados em bytecode sejam executados em qualquer plataforma que possua uma implementação da JVM, abstraindo as particularidades de cada sistema operacional. A linguagem fundamenta-se nos paradigmas de programação orientada a objetos, promovendo encapsulamento, herança e polimorfismo como pilares para a construção de software modular e reutilizável. Adicionalmente, Java incorpora gerenciamento automático de memória através do coletor de lixo (garbage collector), reduzindo erros relacionados a vazamentos de memória e aumentando a segurança e confiabilidade dos sistemas desenvolvidos (SIERRA; BATES, 2005). Ao longo das décadas, a linguagem evoluiu significativamente, incorporando recursos modernos como expressões lambda, streams e programação funcional a partir do Java 8, mantendo-se relevante diante das demandas tecnológicas atuais. O ecossistema Java caracteriza-se pela diversidade de frameworks e bibliotecas que facilitam o desenvolvimento de aplicações robustas e escaláveis. Entre as soluções mais relevantes, destaca-se o Spring Framework, surgido em 2002 como resposta à complexidade dos Enterprise JavaBeans (EJB), oferecendo uma abordagem mais leve e flexível baseada em Inversão de Controle e Injeção de Dependências. O Spring Framework revolucionou a maneira como aplicações Java são estruturadas, promovendo baixo acoplamento entre componentes e facilitando a testabilidade do código (DEITEL; DEITEL, 2017). A partir de 2014, o projeto Spring Boot emergiu como uma extensão do Spring Framework, com o objetivo de simplificar drasticamente o processo de configuração e inicialização de aplicações. Fundamentado no princípio de convenção sobre configuração, o Spring Boot oferece auto-configuração inteligente, dependências inicializadoras (starters) e servidores de aplicação embutidos, permitindo que desenvolvedores concentrem-se na lógica de negócio em vez de em configurações repetitivas (WALLS, 2016, tradução nossa). Essa abordagem alinha-se às metodologias ágeis de desenvolvimento, nas quais a entrega rápida de valor ao cliente é priorizada, reduzindo significativamente o tempo de desenvolvimento de protótipos e aplicações em produção. A combinação de Java com Spring Boot tornou-se padrão de facto para o desenvolvimento de APIs RESTful, microsserviços e sistemas distribuídos na indústria de tecnologia. O Spring Boot oferece integração nativa com tecnologias amplamente utilizadas, incluindo bancos de dados relacionais e não relacionais, sistemas de mensageria, ferramentas de segurança e plataformas de nuvem, constituindo um ecossistema completo para aplicações modernas (DEITEL; DEITEL, 2017). A presença de uma comunidade ativa e extensa documentação oficial contribui para sua adoção em projetos de diferentes escalas e complexidades. Além disso, a arquitetura promovida pelo Spring Boot facilita a implementação de práticas DevOps, como integração contínua e implantação automatizada, essenciais para a manutenção de sistemas em ambientes de produção dinâmicos. Em síntese, Java e seu ecossistema, especialmente por meio do Spring Boot, consolidam-se como ferramentas indispensáveis para o desenvolvimento de software empresarial moderno, oferecendo produtividade, escalabilidade e alinhamento com as melhores práticas de engenharia de software.

\section{Linguagem React}
\label{sec:linguagem-react}

O ReactJS, frequentemente denominado apenas React, é uma biblioteca JavaScript de código aberto amplamente empregada no desenvolvimento de interfaces de usuário para aplicações \emph{web}. Em sua definição essencial, React é ``uma biblioteca JavaScript para desenvolver interfaces de usuário'' (STEFANOV, 2016, tradução nossa). Diferentemente de \emph{frameworks} mais abrangentes, o React se concentra unicamente na camada de visualização (\emph{View}), adotando uma abordagem declarativa para renderizar dados de modo eficiente e consistente. Em vez de manipular o \emph{Document Object Model} (DOM) de forma imperativa, o desenvolvedor descreve apenas o estado final desejado da interface, cabendo ao React realizar as atualizações necessárias (WIERUCH, 2024, tradução nossa). A arquitetura do React se sustenta em três pilares conceituais principais. O primeiro e mais fundamental é a componentização. Nesse modelo, a interface do usuário é decomposta em blocos independentes e reutilizáveis denominados componentes. Segundo Lerner et al. (2017), os componentes ``são os elementos fundamentais das aplicações React, que capacitam os desenvolvedores a construírem componentes de interface de usuário modulares e passíveis de reutilização''. Cada componente encapsula tanto sua lógica quanto sua apresentação, promovendo a modularidade e a manutenção do código, além de possibilitar a construção de interfaces complexas a partir de unidades menores e coesas. O segundo pilar da arquitetura é o \emph{Virtual} DOM (VDOM), responsável pelo alto desempenho na renderização das interfaces. Para otimizar o processo de atualização da interface, o React utiliza o VDOM como uma representação leve e mantida em memória do DOM real do navegador (STEFANOV, 2016, tradução nossa). Quando o estado de um componente sofre alterações, o React atualiza primeiramente o VDOM e, em seguida, realiza uma operação de \emph{diffing} (comparação) entre a versão anterior e a nova. Apenas as diferenças identificadas são refletidas no DOM real. Essa estratégia reduz significativamente as operações diretas sobre o DOM, conhecidas por serem lentas, resultando em renderizações mais rápidas e eficientes (FLANAGAN, 2020, tradução nossa). O terceiro pilar é o JSX (JavaScript XML), uma extensão de sintaxe que simplifica o desenvolvimento de interfaces no React. De acordo com Wieruch (2024, tradução nossa), o JSX é ``uma extensão de sintaxe para JavaScript usada no React que permite aos desenvolvedores escreverem código semelhante ao HTML diretamente dentro do JavaScript''. Essa característica promove a integração entre a lógica de renderização e a estrutura visual do componente, tornando o código mais legível e favorecendo a compreensão da hierarquia de elementos. A adoção do React em projetos de desenvolvimento web contemporâneos justifica-se por sua eficiência técnica, escalabilidade e maturidade consolidada no mercado. Wieruch (2024, p. 7-8) destaca que, com a ascensão do React, o conceito de componentes popularizou-se no desenvolvimento web, onde cada componente encapsula sua apresentação visual e comportamento, permitindo reutilização em hierarquias complexas para construir aplicações completas. Embora o React mantenha foco como biblioteca de componentes, seu ecossistema circundante o transforma em um framework flexível, caracterizado por uma API enxuta e um ecossistema estável em constante evolução. Nesse contexto, o React evoluiu para além de uma biblioteca de interface, tornando-se um ecossistema completo que suporta o desenvolvimento \emph{full-stack} por meio de integrações com tecnologias complementares, como o Next.js, React Native e diversas soluções de gerenciamento de estado. Conforme destaca Derks (2022, tradução nossa), o React representa atualmente ``um ecossistema integral que viabiliza o desenvolvimento completo (\emph{full-stack}), estendendo suas funcionalidades para além dos limites convencionais do frontend''. Essa abrangência o consolida como uma das tecnologias mais relevantes e adotadas no desenvolvimento web moderno, sustentando aplicações escaláveis, performáticas e com experiências de usuário aprimoradas.

\section{API -- Application Programming Interface}
\label{sec:api}

A arquitetura de software contemporânea é amplamente dependente da comunicação eficiente e padronizada entre diferentes aplicações, um conceito materializado pela API (Application Programming Interface), ou Interface de Programação de Aplicações (ORACLE, 2024). Essencialmente, a API é um contrato de serviço que define um conjunto de regras, protocolos e ferramentas para a interação entre um cliente e um servidor. Sua principal função é atuar como um intermediário, que permite que aplicações troquem dados e usem funcionalidades umas das outras de forma segura e desacoplada, sem que a aplicação cliente precise conhecer a complexidade de implementação interna do servidor (AWS, 2024). Este modelo de separação de responsabilidades é vital para a construção de sistemas distribuídos robustos, como preconizado por Coulouris et al. (2013). O mecanismo de funcionamento de uma API é fundamentado no modelo de requisição-resposta. A aplicação cliente inicia a comunicação enviando uma requisição à API, especificando a ação desejada sobre um recurso. A API, após validação e autorização, encaminha a solicitação ao sistema servidor, que processa a lógica de negócios e devolve os dados resultantes. Por fim, a API formata esses dados, geralmente utilizando a notação leve JSON (JavaScript Object Notation), e os entrega ao cliente como resposta. Na arquitetura web, o estilo dominante para estruturar essa comunicação é o REST (Representational State Transfer). Formalizado por Roy Fielding em sua tese de doutorado no ano 2000, o REST é um conjunto de restrições arquitetônicas que conferem ao sistema características como escalabilidade e confiabilidade (FIELDING, 2000, tradução nossa). As APIs que aderem a essas restrições, denominadas RESTful, utilizam o protocolo HTTP para manipular recursos (identificados por Endpoints - URLs específicas) por meio de métodos padronizados, como GET para leitura e POST para criação. Uma restrição crucial do REST é a ausência de estado (Stateless), exigindo que cada requisição do cliente inclua todas as informações necessárias para ser processada, o que simplifica o gerenciamento de sessões no lado do servidor (FIELDING, 2000, tradução nossa). Em suma, a API se estabelece como um facilitador da interoperabilidade e da inovação. Ao fornecer um ponto de comunicação estável e documentado, as APIs promovem a reutilização de código e a aceleração do desenvolvimento, permitindo que sistemas utilizem serviços complexos de terceiros --- como pagamento, autenticação ou geolocalização --- sem a necessidade de construí-los do zero. Além disso, elas são essenciais para o desacoplamento entre a lógica de negócios e a interface do usuário, assegurando que o sistema seja modular, escalável e apto a ser consumido por múltiplos tipos de clientes (web, mobile, dispositivos IoT), alinhando-se às boas práticas de arquitetura de software distribuído (RICHARDS; FORD, 2020).

\section{Banco de Dados}
\label{sec:banco-dados}

A gestão eficiente de dados constitui um pilar fundamental para sistemas de informação, sendo materializada por meio de Sistemas Gerenciadores de Banco de Dados (SGBDs), que são softwares especializados projetados para criar, manipular e administrar grandes volumes de dados de forma estruturada e segura (ELMASRI; NAVATHE, 2019). Historicamente, a evolução dos SGBDs acompanhou o crescimento exponencial da produção de dados, partindo de sistemas hierárquicos e em rede nas décadas de 1960 e 1970, até a consolidação do modelo relacional, proposto por Edgar F. Codd em 1970, que introduziu fundamentos matemáticos baseados na teoria dos conjuntos e na álgebra relacional (DATE, 2004). Esse modelo estabeleceu-se como padrão da indústria devido à sua capacidade de representar relacionamentos complexos entre entidades de forma intuitiva e flexível. Atualmente, observa-se também o crescimento de bancos de dados não relacionais, conhecidos como NoSQL, que atendem demandas específicas de escalabilidade horizontal e flexibilidade de esquema, características essenciais em aplicações de big data e sistemas distribuídos (SILBERSCHATZ; KORTH; SUDARSHAN, 2020). O modelo relacional organiza informações em tabelas compostas por linhas e colunas, onde cada tabela representa uma entidade do domínio do problema, e as colunas definem os atributos dessa entidade. As relações entre tabelas são estabelecidas por meio de chaves primárias e estrangeiras, garantindo integridade referencial e eliminando redundâncias por meio do processo de normalização (ELMASRI; NAVATHE, 2019). A manipulação desses dados é realizada através da Structured Query Language (SQL), uma linguagem declarativa padronizada pela ISO que permite operações de consulta, inserção, atualização e exclusão de dados de forma eficiente e expressiva. Os SGBDs relacionais implementam o conceito de transações, que são conjuntos de operações executadas de forma atômica, consistente, isolada e durável, propriedades conhecidas pelo acrônimo ACID (DATE, 2004). Essas garantias são cruciais para a confiabilidade de sistemas críticos, como instituições financeiras e sistemas de saúde, onde a integridade dos dados não pode ser comprometida. Além disso, os SGBDs oferecem mecanismos de controle de concorrência, que asseguram que múltiplos usuários possam acessar e modificar dados simultaneamente sem causar inconsistências. A escolha adequada do sistema de banco de dados impacta diretamente a performance, escalabilidade e manutenibilidade de aplicações modernas. Em sistemas web e APIs RESTful, os bancos de dados relacionais frequentemente atuam como camada de persistência, armazenando informações que são expostas através de endpoints HTTP, enquanto bancos NoSQL são preferidos em cenários que demandam alta disponibilidade e particionamento de dados (SILBERSCHATZ; KORTH; SUDARSHAN, 2020). No contexto de Internet das Coisas, a capacidade de processar grandes volumes de dados gerados continuamente por dispositivos distribuídos exige estratégias híbridas que combinam SGBDs relacionais para dados estruturados e soluções NoSQL para dados de telemetria em tempo real. A integração entre bancos de dados e frameworks de desenvolvimento, facilitada por tecnologias como Object-Relational Mapping (ORM), abstrai a complexidade das operações de banco, permitindo que desenvolvedores manipulem dados por meio de objetos de programação, acelerando o desenvolvimento e reduzindo erros (ROB; CORONEL, 2011). Em síntese, os SGBDs são componentes essenciais da arquitetura de software contemporânea, viabilizando a construção de sistemas confiáveis, escaláveis e alinhados às necessidades de negócio.

\section{Internet das Coisas IoT}
\label{sec:iot}

A Internet das Coisas, representa uma tecnologia que possibilita a interconexão de dispositivos físicos por meio de redes de comunicação, permitindo a coleta, transmissão e processamento de dados. Segundo Queiroz et al. (2023), a IoT pode ser compreendida como a infraestrutura que possibilita a conexão e comunicação entre dispositivos inteligentes, com o objetivo de coletar, transmitir e receber informações de maneira coordenada, transformando objetos convencionais em entidades digitais capazes de interagir entre si e com o ambiente. Os sistemas IoT fundamentam-se em três pilares funcionais principais: a conectividade ubíqua, que permite a comunicação contínua entre dispositivos por meio de tecnologias sem fio como Bluetooth, WiFi, LoRa e redes celulares; a capacidade de sensoriamento ambiental, onde sensores convertem grandezas físicas em sinais digitais; e o processamento distribuído de dados, que envolve tanto processamento local nos dispositivos (\emph{edge computing}) quanto processamento centralizado em servidores remotos (\emph{cloud computing}), otimizando latência e consumo de recursos (VASCONCELOS FILHO et al., 2023; QUEIROZ et al., 2023). Os sistemas IoT estruturam-se em arquiteturas em camadas que organizam funcionalidades específicas. Queiroz et al. (2023) propõem uma arquitetura de cinco camadas fundamentais: camada de dispositivo (sensores, atuadores e microcontroladores); camada de comunicação (protocolos de transmissão de dados); camada de rede (infraestrutura de conectividade); camada de aplicação (interface com usuário e lógica de negócio); e camada de segurança (privacidade e confiança na transmissão). Essa estruturação permite modularidade e escalabilidade, facilitando a integração de diferentes tecnologias. A comunicação entre as camadas utiliza protocolos especializados que atendem diferentes requisitos de largura de banda, latência, alcance e consumo energético. Embora protocolos como MQTT (\emph{Message Queuing Telemetry Transport}) e CoAP (\emph{Constrained Application Protocol}) sejam amplamente utilizados, a escolha deve considerar as especificidades de cada aplicação (VASCONCELOS FILHO et al., 2023). Para aplicações que demandam comunicação de longa distância em ambientes externos, tecnologias como LoRa apresentam vantagens significativas em termos de alcance e consumo energético. A aplicação de tecnologias IoT em eventos esportivos tem se consolidado como estratégia para automação de processos e melhoria da precisão de medições. A Maratona de Boston utiliza tecnologia RFID (Radio-Frequency Identification) UHF para cronometragem automática de mais de 30 mil corredores simultaneamente, eliminando a necessidade de intervenção manual. Entretanto, a tecnologia RFID apresenta limitações em termos de alcance (tipicamente inferior a 10 metros) e custo elevado por dispositivo (DTB NFC, 2025). Na Fórmula 1, a IoT atinge níveis ainda mais elevados, com cada veículo equipado com mais de 300 sensores que geram aproximadamente 1,1 milhão de pontos de dados por segundo. Esses dados são transmitidos em tempo real via telemetria wireless, permitindo monitoramento de desempenho mecânico e otimização de estratégias de corrida (AWS SPORTS, 2025; CATAPULT, 2025). Os benefícios da aplicação de IoT em cronometragem incluem: precisão aumentada na medição de tempos, eliminando variações decorrentes de erro humano; automação de processos, reduzindo dependência de comunicação manual; rastreabilidade completa, permitindo registro detalhado com \emph{timestamp} sincronizado; e escalabilidade, facilitando expansão para eventos com maior número de participantes. Os desafios técnicos envolvem conectividade em ambientes externos, onde interferências podem comprometer a comunicação wireless; autonomia energética para operação contínua; confiabilidade operacional, exigindo mecanismos de redundância; e sincronização temporal, crucial para cronometragem precisa em sistemas distribuídos (VASCONCELOS FILHO et al., 2023).

\section{Embarcamento}
\label{sec:embarcamento}

Os sistemas embarcados, constituem sistemas computacionais microprocessados especializados, projetados para executar funções específicas dentro de um dispositivo ou equipamento maior. Segundo Almeida, Moraes e Seraphim (2016), diferentemente dos computadores de propósito geral, os sistemas embarcados caracterizam-se pela integração completa entre \emph{hardware} e \emph{software}, operando de forma dedicada a tarefas predefinidas. Constata-se que esses sistemas se fundamentam em três pilares: especialização funcional, otimização de recursos e operação em tempo real, características que os tornam adequados a aplicações com restrições de espaço físico, consumo energético e capacidade de processamento. Conforme Denardin e Barriquello (2018, p. 38), os sistemas embarcados são completamente encapsulados e dedicados ao dispositivo ou sistema que controlam, realizando um conjunto de tarefas predefinidas com requisitos específicos. Verifica-se que esses sistemas apresentam atributos distintos que os diferenciam de sistemas computacionais tradicionais, destacando-se a operação com recursos limitados de memória e processamento, a resposta a eventos externos em tempo real e a necessidade de alta confiabilidade e estabilidade. Ademais, esses sistemas operam frequentemente em ambientes restritos ou hostis, demandando robustez e eficiência energética. Observa-se ainda que grande parte dos sistemas embarcados são projetados para funcionar continuamente, sem intervenção humana, executando suas funções de forma autônoma e ininterrupta. A arquitetura de um sistema embarcado compreende elementos fundamentais de hardware e software integrados para atender a requisitos funcionais específicos. No âmbito do hardware, Almeida, Moraes e Seraphim (2016) destacam que os microcontroladores constituem o componente central desses sistemas, integrando em um único chip a unidade de processamento, memória e periféricos de entrada e saída. Diferentemente dos microprocessadores, que demandam componentes externos para operação completa, os microcontroladores oferecem uma solução compacta e econômica, ideal para aplicações com restrições de espaço e custo. Quanto à memória, verifica-se a presença de memória volátil para processamento de dados temporários e memória não volátil para armazenamento permanente do programa e configurações do sistema. As arquiteturas clássicas de sistemas embarcados baseiam-se em dois modelos fundamentais. A arquitetura de von Neumann caracteriza-se pelo armazenamento de instruções e dados em um mesmo espaço de endereçamento, compartilhando um único barramento de comunicação. Em contraste, a arquitetura de Harvard emprega espaços de endereçamento distintos para programa e dados, possibilitando acesso simultâneo e, consequentemente, maior desempenho em aplicações que demandam processamento intensivo. Observa-se que a escolha entre essas arquiteturas depende dos requisitos específicos da aplicação, considerando fatores como velocidade de processamento, complexidade do sistema e restrições de custo. O desenvolvimento de software para sistemas embarcados apresenta particularidades que o distinguem da programação convencional. Segundo Almeida, Moraes e Seraphim (2016, p. 23), as linguagens de programação mais empregadas nesse contexto são C, C++ e Assembly, selecionadas por sua eficiência, controle próximo do hardware e capacidade de gerenciar recursos limitados de forma otimizada. A linguagem C destaca-se por sua grande utilização na comunidade de desenvolvedores embarcados, oferecendo equilíbrio entre controle de baixo nível e produtividade de desenvolvimento. Adicionalmente, linguagens como Python aparecem como alternativas para prototipagem rápida, embora com maior consumo de recursos computacionais. O ciclo de desenvolvimento de sistemas embarcados compreende etapas distintas e iterativas. Inicialmente, o código-fonte é escrito em uma linguagem de programação e, posteriormente, compilado por ferramentas específicas que geram código executável adequado ao microcontrolador alvo. O código compilado é então transferido para a memória não volátil do dispositivo através de interfaces de programação especializadas. A crescente integração dos sistemas embarcados com tecnologias de conectividade, especialmente no contexto da Internet das Coisas, expande significativamente suas possibilidades de aplicação. Denardin e Barriquello (2018) destacam que os sistemas embarcados são a espinha dorsal de inovações tecnológicas modernas, presentes em dispositivos que variam desde eletrodomésticos até veículos autônomos e equipamentos médicos críticos. Esses sistemas executam funções específicas com restrições temporais bem definidas, interagindo com eventos externos síncronos ou assíncronos, características essenciais para aplicações que demandam determinismo e confiabilidade. Na questão de eventos e monitoramento em tempo real, os sistemas embarcados viabilizam aplicações diversas. Em eventos esportivos, dispositivos embarcados integrados a sensores possibilitam rastreamento de atletas, monitoramento de condições ambientais e coleta automática de dados de desempenho. Na automação industrial, controlam processos críticos com alta confiabilidade e resposta em tempo real. Em dispositivos médicos garantem operação precisa e segura, processando sinais vitais continuamente. Entretanto, o desenvolvimento e implementação de sistemas embarcados apresentam desafios consideráveis. Destacam-se as restrições de recursos computacionais, demandando otimização rigorosa do código e eficiência no uso de memória e processamento. A necessidade de operação confiável em ambientes adversos exige robustez no design de hardware e software. Ademais, em aplicações conectadas, aspectos de segurança da informação tornam-se críticos, requerendo implementação de mecanismos de criptografia, autenticação e atualizações seguras de firmware.

\section{LoRa}
\label{sec:lora}

LoRa, abreviação de Long Range, constitui tecnologia de radiofrequência sem fio de longo alcance e baixo consumo energético, desenvolvida especificamente para aplicações de Internet das Coisas, possibilitando enlaces de dados extremamente longos que alcançam até cinco quilômetros em áreas urbanas e quinze quilômetros ou mais em ambientes rurais com linha de visada (SEMTECH CORPORATION, 2024, p. 4, tradução nossa). A tecnologia LoRa compreende a camada física de comunicação baseada na modulação \emph{Chirp} \emph{Spread Spectrum} (CSS), técnica derivada de aplicações militares desenvolvidas originalmente para sistemas de radar na década de 1940, que utiliza pulsos \emph{chirp} modulados para codificar dados, demonstrando especial resiliência contra interferência, efeitos Doppler e multipercurso (SEMTECH CORPORATION, 2015, p. 9; LEA, 2018, tradução nossa). A modulação LoRa, patenteada pela Semtech em 2014, estabelece compromisso entre sensibilidade e taxa de dados operando em larguras de banda fixas de 125 kHz ou 500 kHz, permitindo que sinais \emph{chirp} possuam amplitude constante e atravessem toda a largura de banda de forma linear, proporcionando robustez mesmo em ambientes com elevado nível de ruído eletromagnético (HAXHIBEQIRI et al., 2018; SEMTECH CORPORATION, 2024, p. 7, tradução nossa). O consumo energético ultrabaixo constitui característica fundamental da tecnologia, permitindo que dispositivos alimentados por bateria alcancem vida útil de até dez anos (SEMTECH CORPORATION, 2024, p. 4-5, tradução nossa). A tecnologia LoRa fundamenta-se em princípios que priorizam eficiência energética e alcance ao custo da taxa de transferência de dados. Conforme Bertoleti (2019, p. 15), dispositivos LoRa operam em faixas de frequência não licenciadas, conhecidas como bandas ISM (\emph{Industrial, Scientific and Medical}), que variam conforme regulamentações regionais. No Brasil, a Agência Nacional de Telecomunicações homologou as faixas de 902 a 907,5 MHz e 915 a 928 MHz para operação de dispositivos LoRa. Nota-se que as taxas de transmissão variam entre 0,3 e 50 kbps, característica ideal para aplicações que necessitam realizar envios constantes de pequenos pacotes de dados. LoRaWAN, vem de Long Range Wide Area Network, e compreende protocolo de rede de área ampla que opera sobre a camada física LoRa, definindo a arquitetura do sistema e os mecanismos de comunicação entre dispositivos. Segundo Bertoleti (2019, p. 18), LoRaWAN constitui protocolo aberto desenvolvido e mantido pela LoRa Alliance, organização global que padroniza e certifica dispositivos compatíveis. A arquitetura de rede LoRaWAN adota topologia em estrela, onde os dispositivos finais comunicam-se diretamente com gateways, que por sua vez encaminham os dados para um servidor de rede central através da internet. Esta configuração elimina a necessidade de sincronização entre nós e reduz o consumo energético dos dispositivos terminais. Nas especificações, LoRaWAN define três classes de dispositivos baseados em suas características operacionais. A Classe A representa a configuração padrão de menor consumo energético, onde os dispositivos transmitem dados de forma assíncrona e abrem janelas de recepção apenas após transmissões próprias, sendo obrigatoriamente implementada por todos os dispositivos LoRaWAN. A Classe A permite comunicação bidirecional onde cada transmissão ascendente do dispositivo é seguida por duas janelas curtas de recepção descendente, constituindo a configuração de menor consumo energético e o conjunto básico de opções obrigatórias que todo dispositivo final necessita implementar. A Classe B estende capacidades da Classe A abrindo periodicamente janelas extras de recepção denominadas \emph{ping slots} para receber mensagens descendentes, utilizando \emph{beacons} sincronizados para derivar e alinhar seus relógios internos com a rede. A Classe C mantém janelas de recepção continuamente abertas exceto durante transmissões, possibilitando receber mensagens descendentes a qualquer momento e oferecendo latência muito baixa para enlaces servidor-dispositivo (LORA ALLIANCE, 2018, p. 9; HAXHIBEQIRI et al., 2018, tradução nossa). A rede transmite \emph{beacons} a cada 128 segundos para dispositivos Classe B, garantindo comunicação em horários programados com melhor previsibilidade de latência (JABBAR et al., 2022, tradução nossa). Dispositivos Classe C não dependem de alimentação por bateria e operam continuamente ativos, resultando em consumo energético significativamente maior, sendo adequados para aplicações alimentadas diretamente por rede elétrica (SEMTECH CORPORATION, 2024, p. 25, tradução nossa). Esta classificação hierárquica permite adaptação da tecnologia conforme requisitos específicos da aplicação, estabelecendo equilíbrio entre consumo energético, latência de comunicação e responsividade do sistema (JABBAR et al., 2022, tradução nossa). A segurança constitui aspecto fundamental na especificação LoRaWAN. Bertoleti (2019, p. 21) destaca que o protocolo implementa criptografia AES (\emph{Advanced Encryption Standard}) de cento e vinte e oito bits em duas camadas distintas. A \emph{Application Session Key} protege o conteúdo dos dados entre dispositivo e servidor de aplicação, garantindo confidencialidade das informações transmitidas. A \emph{Network Session Key} garante a integridade e autenticidade das mensagens, permitindo que o servidor de rede valide pacotes sem acessar seu conteúdo. Cada dispositivo possui identificador único, e o processo de ativação na rede demanda autenticação mútua, prevenindo acesso não autorizado. Em contrapartida, a tecnologia possui limitações intrínsecas em seu design. A baixa taxa de transferência de dados torna LoRa inadequada para aplicações que demandam transmissão contínua de grandes volumes de informação. Ademais, a natureza assíncrona da comunicação introduz latência que pode ser incompatível com sistemas de controle em tempo real de natureza crítica. Regulamentações de uso de espectro impõem restrições adicionais, limitando o tempo de transmissão por dispositivo para evitar saturação das faixas não licenciadas.

\section{ESP32}
\label{sec:esp32}

O ESP32 constitui microcontrolador desenvolvido pela Espressif Systems em 2016 como sucessor do ESP8266. Trata-se de sistema computacional completo integrado em único \emph{chip} de baixo custo e baixo consumo energético que integra processador \emph{dual-core} Tensilica Xtensa LX6 de 32 bits operando a até 240 MHz, incorporando capacidade de processamento, memória e módulos de comunicação sem fio WiFi e Bluetooth (OLIVEIRA, 2021), característica que o posiciona como solução adequada para desenvolvimento de aplicações em Internet das Coisas. A família de microcontroladores ESP8266 e ESP32 da Espressif constitui opção extremamente interessante para aplicações de Internet das Coisas devido ao custo reduzido e aos recursos suficientes para diversas implementações IoT (OLIVEIRA, 2021). O ESP32 caracteriza-se pela alta integração, incorporando comutadores de antena, balun RF, amplificador de potência, amplificador de recepção de baixo ruído, filtros e módulos de gerenciamento de energia, o que o posiciona como solução adequada para desenvolvimento de dispositivos conectados em Internet das Coisas (ESPRESSIF SYSTEMS, 2025). Na questão dos recursos de hardware, o ESP32 apresenta memória integrada suficiente para armazenamento de código de aplicação e dados operacionais. Disponibiliza múltiplos pinos de entrada e saída digitais configuráveis, conversores que permitem leitura de sensores analógicos e geração de sinais de controle modulados. Segundo Bertoleti (2019, p. 42), o dispositivo oferece diversas interfaces de comunicação padrão da indústria eletrônica, permitindo conexão com ampla variedade de sensores, \emph{displays} e outros periféricos. O módulo WiFi permite conexão com redes domésticas e corporativas, possibilitando comunicação com internet e serviços em nuvem. O módulo Bluetooth integrado suporta tanto comunicação tradicional quanto modo de baixo consumo energético, viabilizando conexão com dispositivos móveis e periféricos diversos. Conforme Oliveira (2020, p. 91), esta dupla conectividade permite que o microcontrolador atenda simultaneamente requisitos de comunicação de longo alcance e proximidade, característica particularmente vantajosa em aplicações que demandam múltiplos canais de interação. O desenvolvimento de software para ESP32 beneficia-se de ecossistema abrangente de ferramentas. O ambiente oficial desenvolvido pela fabricante, baseado em sistema operacional de tempo real FreeRTOS, provê acesso completo aos recursos de hardware. Alternativamente, apresenta compatibilidade com plataforma Arduino, ambiente amplamente conhecido que facilita prototipagem rápida e reduz curva de aprendizado. Conforme Bertoleti (2019, p. 35), esta versatilidade de ambientes de desenvolvimento torna o microcontrolador acessível tanto para desenvolvedores experientes quanto para iniciantes na área de sistemas embarcados. No contexto de aplicações práticas, o ESP32 demonstra versatilidade em automação e Internet das Coisas. Em automação residencial, viabiliza controle remoto de iluminação, monitoramento de temperatura e umidade, além de integração com assistentes virtuais. Em ambientes industriais, possibilita aquisição de dados de sensores e comunicação com sistemas de supervisão. Em aplicações veiculares, permite interface com sistemas eletrônicos automotivos para diagnóstico e telemetria. Para dispositivos portáteis alimentados por bateria, oferece modos de operação de baixíssimo consumo energético, possibilitando autonomia prolongada quando o sistema permanece em estado de espera, acordando periodicamente para realizar medições e transmissões. Aspectos de segurança integram a arquitetura do dispositivo. Implementa recursos de criptografia acelerados por hardware, permitindo proteção eficiente de dados transmitidos e armazenados. Disponibiliza mecanismos para geração de números aleatórios seguros e proteção de código armazenado na memória. O suporte a protocolos de comunicação segura possibilita conexões protegidas com servidores remotos, aspecto crítico em aplicações conectadas à nuvem que manipulam informações sensíveis. Entretanto, o dispositivo apresenta limitações que devem ser consideradas. A necessidade de alimentação em tensão específica demanda circuitos reguladores adicionais quando integrado a sistemas que operam em tensões diferentes. O consumo energético durante transmissão contínua de dados sem fio, embora otimizado, pode comprometer a autonomia de aplicações alimentadas exclusivamente por bateria. Ademais, a riqueza de recursos disponíveis exige investimento maior em aprendizado comparado a microcontroladores de arquitetura mais simples. Não obstante, a integração de processamento paralelo, conectividade WiFi e Bluetooth nativa, amplo ecossistema de desenvolvimento e custo reduzido consolidam o ESP32 como alternativa preferencial para implementação de sistemas embarcados conectados em aplicações de Internet das Coisas e automação.
